{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ##KNN Classification example on credit data -- \
#http://nbviewer.jupyter.org/github/arqmain/Machine_Learning/blob/master/R_MLearning/MLearning_Classification_Comparison_R_MLR_KFold/Project3_RProject_MLR_CVKFold.ipynb\
\
#rename columns to english\
\
# read and attach the dataset\
filename <- "http://www.arqmain.net/MLearning/Datasets/Loan2016last.csv"\
\
# load the CSV file from the local directory\
df <- read.csv(filename, header=TRUE)\
attach(df)\
\
library(mlr)\
\
summarizeColumns(df)\
\
# This is a map for visualizing the missing rows (Package Amelia).\
#library(Amelia)\
#missmap(df)\
\
#stratified sampling to get the train and test dataset\
#install.packages("caret")\
library(caret)\
set.seed(7) # Set Seed so that same sample can be reproduced in future also\
# considering response variable as strata\
data_part <- createDataPartition(y = df$Tclient,  p = 0.70, list = F)\
test <- df[-data_part,] # 30% data goes here\
train <- df[data_part,] # 70% here\
\
ltrain = length(train$Tclient)\
ltest = length(test$Tclient)\
\
perc_test = ltest / (ltest + ltrain)\
perc_train = ltrain / (ltest + ltrain)\
perc_test\
perc_train\
\
#summarize train and test dataset by columns \
head(train)\
head(test)\
\
summarizeColumns(train)\
summarizeColumns(test)\
\
#description of train and test data\
trainTask <- makeClassifTask(data = train,target = "Tclient")\
testTask <- makeClassifTask(data = test, target = "Tclient")\
\
#It is considering positive class as Bad, whereas it should be Good. Let\'92s modify it:\
trainTask <- makeClassifTask(data = train,target = "Tclient", positive = "Good")\
\
trainTask\
\
str(getTaskData(trainTask))\
\
#K-Nearest Neighbors\
lrn3 = makeLearner("classif.knn", predict.type = "response")\
\
#cross validation (cv) metrics\
learner = lrn3\
task = trainTask\
iters = 10\
stratify = TRUE\
measures = list(acc, tpr, tnr, fpr, f1) \
cv.lrn3 <- crossval(learner, task, iters, stratify, measures, show.info = F)\
\
#cross validation metrics summary\
(summa3<-summarizeColumns(cv.lrn3$measures.test[,-1,-8:9]))\
\
#Fit the model (train model)\
mod3 = mlr::train(lrn3, trainTask)  #need to specify mlr to avoid clash with train function of caret\
\
getLearnerModel(mod3)\
\
#Make predictions (predict on train data)\
predTR = predict(mod3, trainTask)\
confusionMatrix(table(predTR$data$truth,predTR$data$response))\
\
#Make predictions (predict on test data)\
pred3 = predict(mod3, testTask)\
confusionMatrix(table(predTest$data$truth,predTest$data$response))\
\
#knn using the class package\
#Note... you cannot put the repsonse in teh train and test arguments.  \
results = class::knn(train[,c(1:5)],test[,c(1:5)],train$Tclient,k = 3)\
test$TClientPred = results\
confusionMatrix(table(test$Tclient,test$TClientPred))\
\
\
### KNN Regression\
\
train = data.frame(x1 = c(1,1,2,3,4,5,8), x2 = c(5,7,8,9,10,10,11), y = c(3,4,5,5,4,7,9) )\
plot(train$x1,train$x2, main = "Simple KNN Example")\
text(train$x1, train$x2, labels = train$y, pos = 4, col = "blue")\
test = data.frame(x1 = c(6), x2 = c(9), y = c(8))\
knn.reg(train,test, y = train$y)\
\
\
# K Means Clustering \
\
clusters = kmeans(train,centers = 3,nstart = 10)\
train$clusters = clusters$cluster\
train %>% ggplot(aes(x = x1, y = x2, label = clusters)) + geom_text()\
\
# Classification using Kmeans clustering and KNN\
\
test = data.frame(x1 = c(6,7,8,8), x2 = c(9,10,10,11), y = c(8,9,9,7), clusters = c(3,1,1,2))\
results = class::knn(train[,c(1:3)],test[,c(1:3)],train[,4], k = 3)\
test$clustersPred = results\
table(test$clusters,test$clustersPred) # function confusionMatrix only for binary classfication.\
}