---
title: "KNN - President Data"
author: "Chad Madding"
date: "November 2, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is from an amazing artical by Deepanshu Bhalla.

### K Nearest Neighbor : Step by Step Tutorial

https://www.r-bloggers.com/k-nearest-neighbor-step-by-step-tutorial/

### Read Data

We read the CSV file with the help of read.csv command. Here the first argument is the name of the dataset.  The second argument - Header = TRUE or T implies that the first row in our csv file denotes the headings while header = FALSE or F indicates that the data should be read from the first line and does not involves any headings.

```{r Read data, echo=TRUE}
# Read data
data1 = read.csv("US Presidential Data.csv", header = TRUE )
head(data1)
```

```{r load library, echo=TRUE}
# load library
library(caret)
library(e1071)
```

### The Caret Package

Here we will use caret package in order to run knn. Since my dependent variable is numeric here thus we need to transform it to factor using as.factor().

```{r Transforming dependent variable, echo=TRUE}
# Transforming the dependent variable to a factor
data1$Win.Loss = as.factor(data1$Win.Loss)
```

In order to partition the data into training and validation sets we use createDataPartition() function in caret.

Firstly we set the seed to be 101 so that the same results can be obtained. In the createDataPartition() the first argument is the dependent variable, p denotes how much data we want in the training set; here we take 70% of the data in training set and rest in cross validation set, list = F denotes that the indices we obtain should be in form of a vector.

```{r Partitioning the data, echo=TRUE}
#Partitioning the data into training and validation data
set.seed(101)
index = createDataPartition(data1$Win.Loss, p = 0.7, list = F )
train = data1[index,]
validation = data1[-index,]
```

The dimensions of training and validation sets are checked via dim().
See first 6 rows of training dataset-

```{r Explore data, echo=TRUE}
# Explore data
dim(train)
dim(validation)
names(train)
head(train)
head(validation)
```
By default, levels of dependent variable in this dataset is "0" "1". Later when we will do prediction, these levels will be used as variable names for prediction so we need to make it valid variable names.

```{r Setting levels, echo=TRUE}
# Setting levels for both training and validation data
levels(train$Win.Loss) <- make.names(levels(factor(train$Win.Loss)))
levels(validation$Win.Loss) <- make.names(levels(factor(validation$Win.Loss)))
```

Here we are using repeated cross validation method using trainControl. Number denotes either the number of folds and 'repeats' is for repeated 'r' fold cross validation. In this case, 3 separate 10-fold validations are used.

```{r Setting up train controls, echo=TRUE}
# Setting up train controls
repeats = 3
numbers = 10
tunel = 10

set.seed(1234)
x = trainControl(method = "repeatedcv",
                 number = numbers,
                 repeats = repeats,
                 classProbs = TRUE,
                 summaryFunction = twoClassSummary)
```

Using train() function we run our knn; Win.Loss is dependent variable, the full stop after tilde denotes all the independent variables are there. In 'data=' we pass our training set, 'method=' denotes which technique we want to deploy, setting preProcess to center and scale tells us that we are standardizing our independent variables

center : subtract mean from values.
scale : divide values by standard deviation.

trControl demands our 'x' which was obtained via train( ) and tunelength is always an integer which is used to tune our algorithm.

```{r model1, echo=TRUE}
model1 <- train(Win.Loss~. , data = train, method = "knn",
               preProcess = c("center","scale"),
               trControl = x,
               metric = "ROC",
               tuneLength = tunel)

# Summary of model
model1
plot(model1)
```

Finally to make predictions on our validation set, we use predict function in which the first argument is the formula to be applied and second argument is the new data on which we want the predictions.

```{r Validation, echo=TRUE}
# Validation
valid_pred <- predict(model1,validation, type = "prob")

#Storing Model Performance Scores
#install.packages("gplots")
#install.packages("ROCR")
library(ROCR)
pred_val <-prediction(valid_pred[,2],validation$Win.Loss)

# Calculating Area under Curve (AUC)
perf_val <- performance(pred_val,"auc")
perf_val

# Plot AUC
perf_val <- performance(pred_val, "tpr", "fpr")
plot(perf_val, col = "green", lwd = 1.5)

#Calculating KS statistics
ks <- max(attr(perf_val, "y.values")[[1]] - (attr(perf_val, "x.values")[[1]]))
ks
```

The Area under curve (AUC) on validation dataset is 0.8670378.
