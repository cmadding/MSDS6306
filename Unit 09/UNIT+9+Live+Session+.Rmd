---
title: "UNIT 9 Live Session"
author: "Bivin"
date: "10/24/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1

```{r}

x_1 = rnorm(1000,5,7) #normal distribution of explanatory variable ... not an assumption of regression
hist(x_1,col = "grey") #histogram of EV
true_error = rnorm(1000,0,2) #generating random error ~ N(0,2)
true_beta_0 = 1.1 #Beta_0
true_beta_1 = -8.2 #Beta_1

y = true_beta_0 + true_beta_1*x_1 + true_error #Generating Repsonses...

hist(y) #view distribuion of Repsonse.... not an assumption of regresssion 

plot(x_1,y, pch = 20, col = "red") #scatter plot of y versus x_1

df = data.frame(x_1 = x_1, y = y) #create a dataframe to use in lm function 

fit = lm(y~x_1,data = df) # fit the model to get Beta_hat_0 and Beta_hat_1
summary(fit) # view the parameter estimate table
confint(fit)
```




##Cross Validation Linear regression model: y = 1.1 - 1.2*x_1 + 4*x_2
# We will randomly split the data into a training set (70%) and test set (30%)
# We will fit the model on the training set and assess its performance by how well it recovers the y's in the test set.
# The loss function will be the MSE .. mean square error
```{r}
MSEholderTrain1 = c()
MSEholderTest1 = c()
MSEholderTrain2 = c()
MSEholderTest2 = c()

samplesize = 500
x_1 = rnorm(samplesize,2,1)
x_2 = rgamma(samplesize,1,2)

par(mfrow = c(1,2))
hist(x_1)
hist(x_2, col = "blue")

#parameter setting ... these are the actual parameters that you would not know in a real world setting
true_beta_0 = 1.1
true_beta_1 = -1.2
true_beta_2 = 4

# error ~ N(0,2)  therefore Var(error) = 4 (noise in the data)
true_error = rnorm(samplesize,0,5)  

#Generate Repsonses
y1 = true_beta_0 + true_beta_1*x_1 + true_beta_2*x_2 + true_error

# Make a dataframe to fit the model
df1 = data.frame(x_1 = x_1, x_2 = x_2, y1 = y1)

#Divide into training and test set ... this one is 75% training 25% test
train_perc = .6
train_indices = sample(seq(1,samplesize,length = samplesize),train_perc*samplesize)
train1 = df1[train_indices,]
test1 = df1[-train_indices,]

#Fit the correct model
fitTrain1 = lm(y1~x_1 + x_2, data = train1)
# These are the predictions of the model on the data that were used to fit the model.
predsTrain1 = predict(fitTrain1)
# These are the predictions of the model on the data that were NOT used to fit the model.
# This is a better measure of how the model will perform in real life
predsTest1 = predict(fitTrain1, newdata = test1)

# Calculation of the MSE for the training set
MSEholderTrain1 = sum((predsTrain1 - train1$y1)^2)/(length(train1$y1) - 2)
# Calculation of the MSE for the Test set
MSEholderTest1 = sum((predsTest1 - test1$y1)^2)/(length(test1$y1) - 2)

#Fit the wrong model
fitTrain2 = lm(y1~x_1, data = train1)
# These are the predictions of the model on the data that were used to fit the model.
predsTrain2 = predict(fitTrain2)
# These are the predictions of the model on the data that were NOT used to fit the model.
# This is a better measure of how the model will perform in real life
predsTest2 = predict(fitTrain2, newdata = test1)


# Calculation of the MSE for the training set
MSEholderTrain2 = sum((predsTrain2 - train1$y1)^2)/(length(train1$y1) - 2)
# Calculation of the MSE for the Test set
MSEholderTest2 = sum((predsTest2 - test1$y1)^2)/(length(test1$y1) - 2)

MSEholderTrain1
MSEholderTest1

MSEholderTrain2
MSEholderTest2

summary(fitTrain1)
summary(fitTrain2)
```

##Cross Validation Linear regression model: y = 1.1 - 1.2*x_1 + 4*x_2
# We will randomly split the data into a training set (70%) and test set (30%)
# We will fit the model on the training set and assess its performance by how well it recovers the y's in the test set.
# The loss function will be the MSE .. mean square error
# We will do this for different sample sizes 50, 100, 150, 200, .... 500.
# At the end we will make a plot of the the MSE v. sample size for 3 different models.  
# As sample size gets larger, the 
```{r}
MSEholderTrain1 = c()
MSEholderTest1 = c()
MSEholderTrain2 = c()
MSEholderTest2 = c()

for( samplesize in seq(50,500,50))
{

x_1 = rnorm(samplesize,2,1)
x_2 = rgamma(samplesize,1,2)

#parameter setting ... these are the actual parameters that you would not know in a real world setting
true_beta_0 = 1.1
true_beta_1 = -1.2
true_beta_2 = 4

# error ~ N(0,2)  therefore Var(error) = 4
true_error = rnorm(samplesize,0,5)

#Generate Repsonses
y1 = true_beta_0 + true_beta_1*x_1 + true_beta_2*x_2 + true_error

# Make a dataframe to fit the model
df1 = data.frame(x_1 = x_1, x_2 = x_2, y1 = y1)

#Divide into training and test set ... this one is 75% training 25% test
train_perc = .7
train_indices = sample(seq(1,samplesize,length = samplesize),train_perc*samplesize)
train1 = df1[train_indices,]
test1 = df1[-train_indices,]

#Fit the correct model
fitTrain1 = lm(y1~x_1 + x_2, data = train1)
# These are the predictions of the model on the data that were used to fit the model.
predsTrain1 = predict(fitTrain1)
# These are the predictions of the model on the data that were NOT used to fit the model.
# This is a better measure of how the model will perform in real life
predsTest1 = predict(fitTrain1, newdata = test1)

# Calculation of the MSE for the training set
MSEholderTrain1[samplesize/50] = sum((predsTrain1 - train1$y1)^2)/(length(train1$y1) - 2)
# Calculation of the MSE for the Test set
MSEholderTest1[samplesize/50] = sum((predsTest1 - test1$y1)^2)/(length(test1$y1) - 2)

MSEholderTrain1
MSEholderTest1

#Fit the wrong model
fitTrain2 = lm(y1~x_1, data = train1)
# These are the predictions of the model on the data that were used to fit the model.
predsTrain2 = predict(fitTrain2)
# These are the predictions of the model on the data that were NOT used to fit the model.
# This is a better measure of how the model will perform in real life
predsTest2 = predict(fitTrain2, newdata = test1)


# Calculation of the MSE for the training set
MSEholderTrain2[samplesize/50] = sum((predsTrain2 - train1$y1)^2)/(length(train1$y1) - 2)
# Calculation of the MSE for the Test set
MSEholderTest2[samplesize/50] = sum((predsTest2 - test1$y1)^2)/(length(test1$y1) - 2)

MSEholderTrain2
MSEholderTest2

}

#plot of MSE Train versus MSE Test across many sizes of the sample 50 - 500.
par(mfrow = c(1,1))
plot(seq(50,500,50), MSEholderTrain1, ylim = c(0,100),type = "l")
lines(seq(50,500,50),MSEholderTest1, ylim = c(0,100),type = "l", col = "blue")

par(mfrow = c(1,1))
plot(seq(50,500,50), MSEholderTest1, ylim = c(0,100),type = "l")
lines(seq(50,500,50),MSEholderTest2, ylim = c(0,100),type = "l", col = "blue")
```








