---
title: "Case Study 2"
author: "Jackson Au & Chad Madding"
date: "November 27, 2018"
output:
 html_document:
   keep_md: yes
 pdf_document: default
 word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(DataExplorer) #fun library to conduct EDA
library(readr)
library(ggplot2)
# the dplyr library is needed for %>%
library(dplyr)
```

    MSDS 6306: Doing Data Science
    Case Study 02
    Due: Sunday, December 9th at 11:59pm. 

#### Description:
DDSAnalytics is an analytics company that specializes in talent management solutions for Fortune 1000 companies. Talent management is defined as the iterative process of developing and retaining employees. It may include workforce planning, employee training programs, identifying high-potential employees and reducing/preventing voluntary employee turnover (attrition). To gain a competitive edge over its competition, DDSAnalytics is planning to leverage data science for talent management. The executive leadership has identified predicting employee turnover as its first application of data science for talent management. Before the business green lights the project, they have tasked your data science team to conduct an analysis of existing employee data.

```{r read in data, echo=TRUE}
#read in data

#setwd("/Users/jackson/Desktop/CaseStudy2/")
#setwd("/Box Sync/Documents/Data Sciance/GitHub/MSDS6306/MSDS6306/Case Study 2")

dfTrain <- read.csv("CaseStudy2-data.csv", header=T,na.strings=c(""))
dfVal <- read.csv("CaseStudy2Validation.csv", header=T,na.strings=c(""))

#data prep and cleaning
#check dfTrain for NAs
sapply(dfTrain,function(x) sum(is.na(x)))
#check dfVal for NAs
sapply(dfVal,function(x) sum(is.na(x)))

#we can drop ID, EmployeeCount, EmployeeNumber, Over18, StandardHours as they don't seem to important explanatory variables
dfTrain <- subset(dfTrain,select=c(2,3,4,5,6,7,8,9,12,13,14,15,16,17,18,19,20,21,22,24,25,26,27,29,30,31,32,33,34,35,36,37))
#dropping the same from the validation set as well
dfVal <- subset(dfVal,select=c(2,3,4,5,6,7,8,9,12,13,14,15,16,17,18,19,20,21,22,24,25,26,27,29,30,31,32,33,34,35,36,37))
```

#### Goal:
Conduct exploratory data analysis (EDA) to determine factors that lead to attrition.

```{r exploratory data analysis, echo=TRUE}
#Variables
#looking at the structure of the data
str(dfTrain)
#the dimensions of the data
dim(dfTrain)
#looking at the distribution of the data
#high-level view of continuous variables
plot_histogram(dfTrain)
plot_density(dfTrain)
#high-level view of categorical variables
#Categorical Variables-Barplots
plot_bar(dfTrain)
```

#### We will now start looking for the correlations in the data between variables. Note: We're including a dummy variable for 'Attrition' to see if any of the variables are highly correlated with 'Attrition' itself!
```{r looking for the correlations in the data, echo=TRUE}
#setting up the data for the correlations graph
dummyvar_dfTrain <- dfTrain[,c(1,4,6,7,9,11,13,15,17,19,21:32)]
dummyvar_Attrition = as.numeric(dfTrain$Attrition)- 1
dummyvar_dfTrain = cbind(dummyvar_Attrition, dummyvar_dfTrain)
#looking at the structure of the data
str(dummyvar_dfTrain)
#looking at the correlations in the data
library(corrplot)
corr_matrix <- cor(dummyvar_dfTrain)
corrplot(corr_matrix, method="circle")
```

#### Next we will find out how many correlations are larger than 0.7 as this typically signals high correlation.
```{r correlations bigger than 0.70, echo=TRUE}
#this func will give us correlations bigger than 0.70
val = 0
for(i in 1:23){
  for(r in 1:23){
    if(corr_matrix[i,r]> 0.70 & i != r){
      val= val + 1
    }
  }  }
  print(val/2)
```

##### There are 7 explanatory variables that are over 0.7 in terms of Pearson's R coefficient.
##### Here is the summery of thoses correlations:
1. The most outstanding result is between Job Level and Monthly income, whose correlation is 0.95.
2. A higher performance rating shows a more Percent salary hike, whose correlation is 0.772.
3. The more years working for the company, the higher Job Levels, whose correlation is 0.77.
4. The more total years working for the company, the higher their monthly income, whose correlation is 0.77.
5. The more years with their current manager, the more years they were at the company, whose correlation is 0.763.
6. The last two show more logical trends. The more years at the company, the more years they are in their current role, whose correlation is 0.753.
7. Lastly, the more years with their current manager, the more years in their current role, whose correlation is 0.71.

#### We will make some observations about some variables and their subjectivity to attrition.
```{r break down those top 7, echo=TRUE}
# Overtime vs Attiriton
ot <- ggplot(dfTrain, aes(OverTime,fill = Attrition))
ot <- ot + geom_histogram(stat="count")
print(ot)
ot_rate <- tapply(as.numeric(dfTrain$Attrition) - 1 ,dfTrain$OverTime,mean)

# Attrition Rate based on Overtime worked (Yes|No)
barplot(ot_rate, xlab = "Worked Overtime?", ylab = "Attrition Rate", col = "turquoise3", ylim = c(0, .3))
```
This chart shows that people who work overtime have more attrition.

```{r MaritalStatus,echo=TRUE}
### MaritalStatus vs Attiriton
ms <- ggplot(dfTrain, aes(MaritalStatus,fill = Attrition))
ms <- ms + geom_histogram(stat="count")
print(ms)
ms_rate <- tapply(as.numeric(dfTrain$Attrition) - 1 ,dfTrain$MaritalStatus,mean)

# Attrition Rate by Marital Status
barplot(ms_rate, xlab = "Marital Status", ylab = "Attrition Rate", col = "turquoise3")
```
Single people have more tendency to be subject to attrition.

```{r JobRole, echo=TRUE}
###JobRole vs Attrition
jr <- ggplot(dfTrain, aes(JobRole,fill = Attrition))
jr <- jr + geom_histogram(stat="count") +
  theme(axis.text.x=element_text(angle=90,hjust=1))
print(jr)
jr_rate <- tapply(as.numeric(dfTrain$Attrition) - 1 ,dfTrain$JobRole,mean)
mean(as.numeric(dfTrain$Attrition) - 1)

#Job Role Attrition Rate
barplot(jr_rate, xlab = "", ylab = "Attrition Rate", col = "turquoise3", ylim = c(0,.5), cex.names=.4, las=2)
```
Here we can see that sales representative roles, human resources workers, laboratory technicians and sales executive have more attrition than other roles given in the data set.

```{r}
###Gender vs Attrition
g <- ggplot(dfTrain, aes(Gender,fill = Attrition))
g <- g + geom_histogram(stat="count")
print(g)
g_rate <- tapply(as.numeric(dfTrain$Attrition) - 1 ,dfTrain$Gender,mean)

# Attrition Rate by Gender
barplot(g_rate, xlab = "Gender", ylab = "Attrition Rate", col = "turquoise3", ylim = c(0,.2))
```
Gender doesn't seem to play much of a role in attrition.

```{r}
###EducationField vs Attrition
ef <- ggplot(dfTrain, aes(EducationField,fill = Attrition))
ef <- ef + geom_histogram(stat="count") +
  theme(axis.text.x=element_text(angle=45,hjust=1))
print(ef)
tapply(as.numeric(dfTrain$Attrition) - 1 ,dfTrain$EducationField,mean)
```
Workers holding technical and marketing degrees are outstanding with a high attrition ratio.

```{r}
###Department vs Attrition
dpt <- ggplot(dfTrain, aes(Department,fill = Attrition))
dpt <- dpt + geom_histogram(stat="count")
print(dpt)
tapply(as.numeric(dfTrain$Attrition) - 1 ,dfTrain$Department,mean)
```
Results in this area are showing that the Sales Department has the highest rate of attrition.

```{r}
###BusinessTravel vs Attrition
bt <- ggplot(dfTrain, aes(BusinessTravel,fill = Attrition))
bt <- bt + geom_histogram(stat="count")
print(bt)
bt_rate <- tapply(as.numeric(dfTrain$Attrition) - 1 ,dfTrain$BusinessTravel,mean)

#Attrition Rate based on Business Travel frequency
barplot(bt_rate, xlab = "Business Travel", ylab = "Attrition Rate", col = "turquoise3", ylim = c(0,.3)) 
```
Looking at travel versus non travel, we see that those who travel more frequently have a higher probability of attrition.

```{r Overtime, echo=TRUE}
### x=Overtime, y= Age, z = MaritalStatus , t = Attrition
ggplot(dfTrain, aes(OverTime, Age)) +  
  facet_grid(.~MaritalStatus) +
  geom_jitter(aes(color = Attrition),alpha = 0.4) +  
  ggtitle("x=Overtime, y= Age, z = MaritalStatus , t = Attrition") +  
  theme_light()
```

This graph shows that single people around (~) the age of 35, working overtime are more subject to attrition.

### Graphing to see which Job Role had, on average, the highest job satisfaction
```{r job satisfaction, echo=TRUE}
dfTrain_JobSat <- dfTrain %>% group_by(JobRole) %>% summarise(JobSatisfaction_mean = mean(JobSatisfaction))
jobsat <- ggplot(dfTrain_JobSat, aes(x = reorder(JobRole,-JobSatisfaction_mean),y=JobSatisfaction_mean)) + geom_bar(stat="identity", color = "sky blue", fill = "sky blue") + xlab("Job Role") + ylab("Job Satisfaction Score (avg)") + theme(axis.text.x=element_text(angle=90,hjust=1))
print(jobsat)
```

On average, research scientists had the highest job satisfaction score, while HR had the lowest score.

### Predictions with logistic regression

```{r Predictions with logistic regression, echo=TRUE}
#loading the needed liraries
library(caTools)
library(e1071)
library(glmnet)
```

Setting up both the training and validation data for our first model (because we want to improve later!)

```{r model 1, echo=TRUE}

dfTrain_model1 = dfTrain[,-c(4,8,10,14,17,26)] 
#taking out dailyrate + monthlyincome because it's similar to monthlyrate
#taking out Education + JobRole fields as they could be covered in the Department field. Taking out Gender as the EDA showed an insignificant difference between male and female.
#training seems irrelevant
dfVal_model1 = dfVal[,-c(4,8,10,14,17,26)]

str(dfTrain)

#looking at the structure of both data sets
str(dfTrain_model1)
str(dfVal_model1)

#simplifining the names
train_m1 <- dfTrain_model1
test_m1 <- dfVal_model1
```

We are going to create the first model using glm and we will train the full model.
```{r full model1, echo=TRUE}
#using glm to train the full model
model1_glm_logit <- glm(Attrition ~.,family=binomial(link='logit'),data=train_m1)
#running the test data through the model
predicted_glm_logitM1 <- predict(model1_glm_logit, test_m1, type='response')
predicted_glm_logitM1 <- ifelse(predicted_glm_logitM1 > 0.5,1,0)
summary(model1_glm_logit)
table(test_m1$Attrition, predicted_glm_logitM1)
#Checking the prediction accuracy for m1 (glm_logit)
print((244+15)/300)
```

For our second model we will be using only the statistically significant variables (only *** variables) from the glm call.
```{r model2, echo=TRUE}
#statistically significant variables (only *** variables)
dfTrain_model2 = dfTrain[,c(2,3,6,9,12,15,16,18,19,30)] 
dfVal_model2 = dfVal[,c(2,3,6,9,12,15,16,18,19,30)]

#simplifining the names
train_m2 <- dfTrain_model2
test_m2 <- dfVal_model2

#using glm to train the full model with a logit setting
model2_glm_logit <- glm(Attrition ~.,family=binomial(link='logit'),data=train_m2)
#running the test data through the model
predicted_glm_logitM2 <- predict(model2_glm_logit, test_m2, type='response')
predicted_glm_logitM2 <- ifelse(predicted_glm_logitM2 > 0.5,1,0)
summary(model2_glm_logit)
table(test_m2$Attrition, predicted_glm_logitM2)
#Checking the prediction accuracy for m2
print((250+4)/300)
```
Interesting to see that the statistically significant-only variables did "worse".

Let's do another one since we want the bonus 5 points! This one will use the variables 0.05 and under.
```{r model3, echo=TRUE}

dfTrain_model3 = dfTrain[,c(2,3,6,9,12,14,15,16,19,20,23,25:31)]  #numcowork is #19
dfVal_model3 = dfVal[,c(2,3,6,9,12,14,15,16,19,20,23,25:31)]

str(dfTrain_model3)

#simplifining the names
train_m3 <- dfTrain_model3
test_m3 <- dfVal_model3

#using glm to train the full model with a logit setting
model3_glm_logit <- glm(Attrition ~.,family=binomial(link='logit'),data=train_m3)
#running the test data through the model
predicted_glm_logitM3 <- predict(model3_glm_logit, test_m3, type='response')
predicted_glm_logitM3 <- ifelse(predicted_glm_logitM3 > 0.5,1,0)
summary(model3_glm_logit)
table(test_m3$Attrition, predicted_glm_logitM3)
#Checking the prediction accuracy for m3 (glm_logit)
print((246+17)/300)
```

We are going to try one more model.
```{r model4, echo=TRUE}
#Model4
dfTrain_model4 = dfTrain[,c(2,3,6,9,14,15,16,20,23,25,27,30,31)]  #numcowork is #19
dfVal_model4 = dfVal[,c(2,3,6,9,14,15,16,20,23,25,27,30,31)]

str(dfTrain_model4)

#simplifining the names
train_m4 <- dfTrain_model4
test_m4 <- dfVal_model4

#using glm to train the full model with a logit setting
model4_glm_logit <- glm(Attrition ~.,family=binomial(link='logit'),data=train_m4)
#running the test data through the model
predicted_glm_logitM4 <- predict(model4_glm_logit, test_m4, type='response')
predicted_glm_logitM4 <- ifelse(predicted_glm_logitM4 > 0.5,1,0)
summary(model4_glm_logit)
table(test_m4$Attrition, predicted_glm_logitM4)
#Checking the prediction accuracy for m4 (glm_logit)
print((247+13)/300)
```

Our best model showed to be 3 with a prediction accuracy of 87.7% and an AIC of 751.82. This is the lowest AIC of the 4 models.

```{r export csv, echo=TRUE}
#adding the predictions from model 3 back into the data as a new column
dfVal$Predictions <- predicted_glm_logitM3
#converting 1 and 0 back into Yes and No
dfVal$Predictions <- factor(dfVal$Predictions, levels=c(0,1), labels=c("No", "Yes"))
#creating a data frame with the valaidation data and the predicted data from model 3
dfPreds <- dfVal[,c(2,33)]
# Writeing dfPreds to the working directory as a CSV file
write.csv(dfPreds, file = "dfPreds.csv")
```

In closing, we found model 3 to be the best and the predictions from that model are available in the dfPreds.csv file.

#### Breaking down each model:
Model1: Removing variables that may not make sense (or were redundant) yielded a prediction accuracy of 86.3%  (AIC: 779.9).

Model2: Using statistically significant variables (p-value < 0.001) yielded a prediction accuracy of 84.7% (AIC: 931.12).

Model3: Using statistically significant variable (p-value <0.05) yielded a prediction accuracy of 87.7% (AIC: 751.82).

Model4: Using statistically significant variables (p-value <0.05) and also removing possibly redundant variables yielded a prediction accuracy of: 86.67% (AIC: 798.65).
