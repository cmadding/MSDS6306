---
title: "Case Study 2 EDA only"
author: "Jackson Au & Chad Madding"
date: "November 27, 2018"
output:
 html_document:
   keep_md: yes
 pdf_document: default
 word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#Install if the package doesn't exist 
#install.packages('DataExplorer') 
library(DataExplorer)
library(readr)
library(ggplot2)
```

    MSDS 6306: Doing Data Science
    Case Study 02
    Due: Sunday, December 9th at 11:59pm. 

#### Description:
DDSAnalytics is an analytics company that specializes in talent management solutions for Fortune 1000 companies. Talent management is defined as the iterative process of developing and retaining employees. It may include workforce planning, employee training programs, identifying high-potential employees and reducing/preventing voluntary employee turnover (attrition). To gain a competitive edge over its competition, DDSAnalytics is planning to leverage data science for talent management. The executive leadership has identified predicting employee turnover as its first application of data science for talent management. Before the business green lights the project, they have tasked your data science team to conduct an analysis of existing employee data.

```{r read in data, echo=TRUE}
#read in data
training_attrition <- read.csv("CaseStudy2-data.csv", header=T,na.strings=c(""))
validation_attrition <- read.csv("CaseStudy2Validation.csv", header=T,na.strings=c(""))

#data prep and cleaning
#check for training_attrition for NAs
sapply(training_attrition,function(x) sum(is.na(x)))
#check for training_attrition for NAs
sapply(validation_attrition,function(x) sum(is.na(x)))

#we can drop ID, EmployeeCount, EmployeeNumber, Over18, StandardHours
training_attrition <- subset(training_attrition,select=c(2,3,4,5,6,7,8,9,12,13,14,15,16,17,18,19,20,21,22,24,25,26,27,29,30,31,32,33,34,35,36,37))
#we can drop ID, EmployeeCount, EmployeeNumber, Over18, StandardHours for the test set
validation_attrition <- subset(validation_attrition,select=c(2,3,4,5,6,7,8,9,12,13,14,15,16,17,18,19,20,21,22,24,25,26,27,29,30,31,32,33,34,35,36,37))
```

#### Goal:
Conduct exploratory data analysis (EDA) to determine factors that lead to attrition.

```{r exploratory data analysis, echo=TRUE}

#Variables
#looking at the structure of the data
str(training_attrition)

#the dimensions of the data
dim(training_attrition)

numeric_training_attrition <- training_attrition[,c(1,4,6,7,9,11,13,15,17,19,21:32)]
numeric_Attrition = as.numeric(training_attrition$Attrition)- 1
numeric_training_attrition = cbind(numeric_Attrition, numeric_training_attrition)
str(numeric_training_attrition)
library(corrplot)
M <- cor(numeric_training_attrition)
corrplot(M, method="circle")

#Finding how many correlations are bigger than 0.70
k = 0
for(i in 1:23){
for(r in 1:23){
  if(M[i,r]> 0.70 & i != r){
    k= k + 1
  }
}  }
print(k/2)

### Overtime vs Attiriton
l <- ggplot(training_attrition, aes(OverTime,fill = Attrition))
l <- l + geom_histogram(stat="count")
print(l)
tapply(as.numeric(training_attrition$Attrition) - 1 ,training_attrition$OverTime,mean)

## As seen in the plot, personnels who work over time have more Attrition

### MaritalStatus vs Attiriton
l <- ggplot(training_attrition, aes(MaritalStatus,fill = Attrition))
l <- l + geom_histogram(stat="count")
print(l)
tapply(as.numeric(training_attrition$Attrition) - 1 ,training_attrition$MaritalStatus,mean)

## single personnels have more tendency to be subject to attrition

###JobRole vs Attrition
l <- ggplot(training_attrition, aes(JobRole,fill = Attrition))
l <- l + geom_histogram(stat="count") +
  theme(axis.text.x=element_text(angle=45,hjust=1))
print(l)
tapply(as.numeric(training_attrition$Attrition) - 1 ,training_attrition$JobRole,mean)
mean(as.numeric(training_attrition$Attrition) - 1)

## we see that labaratory technican, human resources and sales representative roles have more attrition.

###Gender vs Attrition
l <- ggplot(training_attrition, aes(Gender,fill = Attrition))
l <- l + geom_histogram(stat="count")
print(l)
tapply(as.numeric(training_attrition$Attrition) - 1 ,training_attrition$Gender,mean)

## gender characteristic doesn't have so much significance

###EducationField vs Attrition
l <- ggplot(training_attrition, aes(EducationField,fill = Attrition))
l <- l + geom_histogram(stat="count") +
  theme(axis.text.x=element_text(angle=45,hjust=1))
print(l)
tapply(as.numeric(training_attrition$Attrition) - 1 ,training_attrition$EducationField,mean)

## Technical Degree and Human Resources are outstanding with high attrition ratio.

###Department vs Attrition
l <- ggplot(training_attrition, aes(Department,fill = Attrition))
l <- l + geom_histogram(stat="count")
print(l)
tapply(as.numeric(training_attrition$Attrition) - 1 ,training_attrition$Department,mean)

## department results aren't saying anything so important

###BusinessTravel vs Attrition
l <- ggplot(training_attrition, aes(BusinessTravel,fill = Attrition))
l <- l + geom_histogram(stat="count")
print(l)
tapply(as.numeric(training_attrition$Attrition) - 1 ,training_attrition$BusinessTravel,mean)

## if businesstravel = non-travel, so little probability to have attrition
## if businesstravel = travel_frequently , more probability to have attrition.

### x=Overtime, y= Age, z = MaritalStatus , t = Attrition
ggplot(training_attrition, aes(OverTime, Age)) +  
  facet_grid(.~MaritalStatus) +
  geom_jitter(aes(color = Attrition),alpha = 0.4) +  
  ggtitle("x=Overtime, y= Age, z = MaritalStatus , t = Attrition") +  
  theme_light()

## we can say that attrition is cumulating in which OverTime =Yes & Marial_Status=Single & Age< 35

### MonthlyIncome vs. Age, by  color = Attrition
ggplot(training_attrition, aes(MonthlyIncome, Age, color = Attrition)) + 
  geom_jitter() +
  ggtitle("MonthlyIncome vs. Age, by  color = Attrition ") +
  theme_light()

#As Age increases, MonthlyIncome tends to increase

#Prediction with logistic regression :

library(caTools)
library(e1071)
library(glmnet)

training_attrition_mydatanew = training_attrition[,-c(6,9,22)]
validation_attrition_mydatanew = validation_attrition[,-c(6,9,22)]

str(training_attrition_mydatanew)
str(validation_attrition_mydatanew)

train <- training_attrition_mydatanew
test <- validation_attrition_mydatanew

model_glm_binomial <- glm(Attrition ~ ., data = train, family='binomial')
predicted_glm_binomial <- predict(model_glm_binomial, test, type='response')
predicted_glm_binomial <- ifelse(predicted_glm_binomial > 0.5,1,0)

#checking out the first model
model_glm_logit <- glm(Attrition ~.,family=binomial(link='logit'),data=train)
predicted_glm_logit <- predict(model_glm_logit, test, type='response')
predicted_glm_logit <- ifelse(predicted_glm_logit > 0.5,1,0)
#print out the model
summary(predicted_glm_binomial)
summary(predicted_glm_logit)

table(test$Attrition, predicted_glm_binomial)
#Checking the prediction accuracy
print((240+13)/300)

table(test$Attrition, predicted_glm_logit)
#Checking the prediction accuracy
print((240+13)/300)

# The prediction accuracy of logistic regression is about 0.84

validation_attrition$PredictedAttrition_binomial <- predicted_glm_binomial
validation_attrition$PredictedAttrition_logit <- predicted_glm_logit

#check out the predicted data
validation_attrition_compair <- validation_attrition[,c(2,33:34)]


# The prediction accuracy of logistic regression is about 0.84

#Continuous Variables
plot_histogram(training_attrition)

plot_density(training_attrition)

#bivariate/multivariate analysis

#Correlation analysis
plot_correlation(training_attrition, type = 'continuous','Attrition')

#Categorical Variables-Barplots
plot_bar(training_attrition)
```

Looking at the data we can see:
the most outstanding result is between JobLevel and Monthly income, whose correlation is 0.95.
the more performance rating, the more Percent salary hike, whose correlation is 0.772
the more total working Years, the more Job Level, whose correlation is 0.77
the more total working Years, the more monthlyIncome, whose correlation is 0.77
the more yearswithcurrmanager, the more yearsatcompany , whose correlation is 0.763
the more yearsatcompany, the more yearsInCurrentRole , whose correlation is 0.753.
the more yearswithcurrmanager, the more yearsincurrentrole, whose correlation is 0.71

The prediction accuracy of logistic regression is about 'print((240+13)/300)'.

Looking at the first full model.
