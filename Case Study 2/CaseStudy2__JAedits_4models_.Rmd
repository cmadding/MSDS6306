---
title: "Case Study 2"
author: "Jackson Au & Chad Madding"
date: "November 27, 2018"
output:
 html_document:
   keep_md: yes
 pdf_document: default
 word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(DataExplorer) #fun library to conduct EDA
library(readr)
library(ggplot2)
```

    MSDS 6306: Doing Data Science
    Case Study 02
    Due: Sunday, December 9th at 11:59pm. 

#### Description:
DDSAnalytics is an analytics company that specializes in talent management solutions for Fortune 1000 companies. Talent management is defined as the iterative process of developing and retaining employees. It may include workforce planning, employee training programs, identifying high-potential employees and reducing/preventing voluntary employee turnover (attrition). To gain a competitive edge over its competition, DDSAnalytics is planning to leverage data science for talent management. The executive leadership has identified predicting employee turnover as its first application of data science for talent management. Before the business green lights the project, they have tasked your data science team to conduct an analysis of existing employee data.

```{r read in data, echo=TRUE}
#read in data

#setwd("/Users/jackson/Desktop/CaseStudy2/")
#setwd("/Box Sync/Documents/Data Sciance/GitHub/MSDS6306/MSDS6306/Case Study 2")

training_attrition <- read.csv("CaseStudy2-data.csv", header=T,na.strings=c(""))
validation_attrition <- read.csv("CaseStudy2Validation.csv", header=T,na.strings=c(""))

#data prep and cleaning
#check training_attrition for NAs
sapply(training_attrition,function(x) sum(is.na(x)))
#check validation_attrition for NAs
sapply(validation_attrition,function(x) sum(is.na(x)))

#we can drop ID, EmployeeCount, EmployeeNumber, Over18, StandardHours as they don't seem to important explanatory variables
training_attrition <- subset(training_attrition,select=c(2,3,4,5,6,7,8,9,12,13,14,15,16,17,18,19,20,21,22,24,25,26,27,29,30,31,32,33,34,35,36,37))
#dropping the same from the validation set as well
validation_attrition <- subset(validation_attrition,select=c(2,3,4,5,6,7,8,9,12,13,14,15,16,17,18,19,20,21,22,24,25,26,27,29,30,31,32,33,34,35,36,37))
```

#### Goal:
Conduct exploratory data analysis (EDA) to determine factors that lead to attrition.

```{r exploratory data analysis, echo=TRUE}
#Variables
#looking at the structure of the data
str(training_attrition)
#the dimensions of the data
dim(training_attrition)
#looking at the distribution of the data
#high-level view of continuous variables
plot_histogram(training_attrition)
plot_density(training_attrition)
#high-level view of categorical variables
#Categorical Variables-Barplots
plot_bar(training_attrition)
```

#### We will now start looking for the correlations in the data between variables. Note: We're including a dummy variable for 'Attrition' to see if any of the variables are highly correlated with 'Attrition' itself!
```{r looking for the correlations in the data, echo=TRUE}
#setting up the data for the correlations graph
dummyvar_training_attrition <- training_attrition[,c(1,4,6,7,9,11,13,15,17,19,21:32)]
dummyvar_Attrition = as.numeric(training_attrition$Attrition)- 1
dummyvar_training_attrition = cbind(dummyvar_Attrition, dummyvar_training_attrition)
#looking at the structure of the data
str(dummyvar_training_attrition)
#looking at the correlations in the data
library(corrplot)
corr_matrix <- cor(dummyvar_training_attrition)
corrplot(corr_matrix, method="circle")
```

#### Next we will find out how many correlations are bigger than 0.7 as this typically signals high correlation.
```{r correlations bigger than 0.70, echo=TRUE}
#this func will give me correlations bigger than 0.70
val = 0
for(i in 1:23){
  for(r in 1:23){
    if(corr_matrix[i,r]> 0.70 & i != r){
      val= val + 1
    }
  }  }
  print(val/2)
```

##### There are 7 explanatory variables that are over 0.7 in terms of Pearson's R coefficient!  We will make some observations about each explanatory variables and their subjectivity to attrition.
```{r break down those top 7, echo=TRUE}
#looking at the top 7 over 0.70
# Overtime vs Attiriton
ot <- ggplot(training_attrition, aes(OverTime,fill = Attrition))
ot <- ot + geom_histogram(stat="count")
print(ot)
tapply(as.numeric(training_attrition$Attrition) - 1 ,training_attrition$OverTime,mean)
```
This chart shows that people who work overtime have more attrition.

```{r MaritalStatus,echo=TRUE}
### MaritalStatus vs Attiriton
ms <- ggplot(training_attrition, aes(MaritalStatus,fill = Attrition))
ms <- ms + geom_histogram(stat="count")
print(ms)
tapply(as.numeric(training_attrition$Attrition) - 1 ,training_attrition$MaritalStatus,mean)
```
Single people have more tendency to be subject to attrition.

```{r JobRole, echo=TRUE}
###JobRole vs Attrition
jr <- ggplot(training_attrition, aes(JobRole,fill = Attrition))
jr <- jr + geom_histogram(stat="count") +
  theme(axis.text.x=element_text(angle=45,hjust=1))
print(jr)
tapply(as.numeric(training_attrition$Attrition) - 1 ,training_attrition$JobRole,mean)
mean(as.numeric(training_attrition$Attrition) - 1)
```
Here we can see that sales representative roles, human resources workers, laboratory technicians and sales executive have more attrition than other roles given in the data set.

```{r}
###Gender vs Attrition
g <- ggplot(training_attrition, aes(Gender,fill = Attrition))
g <- g + geom_histogram(stat="count")
print(g)
tapply(as.numeric(training_attrition$Attrition) - 1 ,training_attrition$Gender,mean)
```
Gender doesn't seem to play much of a role in attrition.

```{r}
###EducationField vs Attrition
ef <- ggplot(training_attrition, aes(EducationField,fill = Attrition))
ef <- ef + geom_histogram(stat="count") +
  theme(axis.text.x=element_text(angle=45,hjust=1))
print(ef)
tapply(as.numeric(training_attrition$Attrition) - 1 ,training_attrition$EducationField,mean)
```
Workers holding technical and marketing degrees are outstanding with a high attrition ratio.

```{r}
###Department vs Attrition
dpt <- ggplot(training_attrition, aes(Department,fill = Attrition))
dpt <- dpt + geom_histogram(stat="count")
print(dpt)
tapply(as.numeric(training_attrition$Attrition) - 1 ,training_attrition$Department,mean)
```
Results in this area are showing that the Sales Department has the highest rate of attrition.

```{r}
###BusinessTravel vs Attrition
bt <- ggplot(training_attrition, aes(BusinessTravel,fill = Attrition))
bt <- bt + geom_histogram(stat="count")
print(bt)
tapply(as.numeric(training_attrition$Attrition) - 1 ,training_attrition$BusinessTravel,mean)
```
Looking at travel versus non travel, we see that those who travel more frequently have a higher probability of attrition.

```{r Overtime, echo=TRUE}
### x=Overtime, y= Age, z = MaritalStatus , t = Attrition
ggplot(training_attrition, aes(OverTime, Age)) +  
  facet_grid(.~MaritalStatus) +
  geom_jitter(aes(color = Attrition),alpha = 0.4) +  
  ggtitle("x=Overtime, y= Age, z = MaritalStatus , t = Attrition") +  
  theme_light()
```

This graph shows that single people around (~) the age of 35, working overtime are subject to attrition.

#### To summarize, here were the correlations above 0.7:
1. The most outstanding result is between Job Level and Monthly income, whose correlation is 0.95.
2. A higher performance rating shows a more Percent salary hike, whose correlation is 0.772.
3. The more years working for the company, the higher Job Levels, whose correlation is 0.77.
4. The more total years working for the company, the higher their monthly income, whose correlation is 0.77.
5. The more years with their current manager, the more years they were at the company, whose correlation is 0.763.
6. The last two show more logical trends. The more years at the company, the more years they are in their current role, whose correlation is 0.753.
7. Lastly, the more years with their current manager, the more years in their current role, whose correlation is 0.71.

### Predictions with logistic regression
```{r Predictions with logistic regression, echo=TRUE}
#loading the needed liraries
library(caTools)
library(e1071)
library(glmnet)

#setting up both the training and validation data for our first model (because we want to improve later!)
training_attrition_model1 = training_attrition[,-c(4,8,10,14,17,26)] 
#taking out dailyrate + monthlyincome because it's similar to monthlyrate
#taking out Education + JobRole fields as they could be covered in the Department field. Taking out Gender as the EDA showed an insignificant difference between male and female.
#training seems irrelevant
validation_attrition_model1 = validation_attrition[,-c(4,8,10,14,17,26)]

str(training_attrition)

#looking at the structure of both data sets
str(training_attrition_model1)
str(validation_attrition_model1)
#simplifining the names
train_m1 <- training_attrition_model1
test_m1 <- validation_attrition_model1
#using glm to train the full model with a binomial setting
model1_glm_binomial <- glm(Attrition ~ ., data = train_m1, family='binomial')
#running the test data through the model
predicted_glm_binomial <- predict(model1_glm_binomial, test_m1, type='response')
predicted_glm_binomial <- ifelse(predicted_glm_binomial > 0.5,1,0)
summary(model1_glm_binomial)
#using glm to train the full model with a logit setting
model1_glm_logit <- glm(Attrition ~.,family=binomial(link='logit'),data=train_m1)
#running the test data through the model
predicted_glm_logit <- predict(model1_glm_logit, test_m1, type='response')
predicted_glm_logit <- ifelse(predicted_glm_logit > 0.5,1,0)
summary(model1_glm_logit)
table(test_m1$Attrition, predicted_glm_binomial)
#Checking the prediction accuracy for m1 (glm_binomial)
print((244+15)/300)
table(test_m1$Attrition, predicted_glm_logit)
#Checking the prediction accuracy for m1 (glm_logit)
print((244+15)/300) 


# Let's try another model using only the statistically significant variables (only *** variables) from the glm call
training_attrition_model2 = training_attrition[,c(2,3,6,9,12,15,16,18,19,30)] 
validation_attrition_model2 = validation_attrition[,c(2,3,6,9,12,15,16,18,19,30)]

#simplifining the names
train_m2 <- training_attrition_model2
test_m2 <- validation_attrition_model2
#using glm to train the full model with a binomial setting
model2_glm_binomial <- glm(Attrition ~ ., data = train_m2, family='binomial')
#running the test data through the model
predicted_glm_binomial <- predict(model2_glm_binomial, test_m2, type='response')
predicted_glm_binomial <- ifelse(predicted_glm_binomial > 0.5,1,0)
summary(model2_glm_binomial)
#using glm to train the full model with a logit setting
model2_glm_logit <- glm(Attrition ~.,family=binomial(link='logit'),data=train_m2)
#running the test data through the model
predicted_glm_logit <- predict(model2_glm_logit, test_m2, type='response')
predicted_glm_logit <- ifelse(predicted_glm_logit > 0.5,1,0)
summary(model2_glm_logit)
table(test_m2$Attrition, predicted_glm_binomial)
#Checking the prediction accuracy for m2 (glm_binomial)
print((250+4)/300)
table(test_m2$Attrition, predicted_glm_logit)
#Checking the prediction accuracy for m2 (glm_logit)
print((250+4)/300)

##***Interesting to see that the statistically significant-only variables did "worse"


## Let's do another one since you want the bonus 5 points! *** represented a significance of <0.001, which is quite harsh. Let's try to do it with variables 0.05 and under
training_attrition_model3 = training_attrition[,c(2,3,6,9,12,14,15,16,19,20,23,25:31)]  #numcowork is #19
validation_attrition_model3 = validation_attrition[,c(2,3,6,9,12,14,15,16,19,20,23,25:31)]

str(training_attrition_model3)

#simplifining the names
train_m3 <- training_attrition_model3
test_m3 <- validation_attrition_model3
#using glm to train the full model with a binomial setting
model3_glm_binomial <- glm(Attrition ~ ., data = train_m3, family='binomial')
#running the test data through the model
predicted_glm_binomial <- predict(model3_glm_binomial, test_m3, type='response')
predicted_glm_binomial <- ifelse(predicted_glm_binomial > 0.5,1,0)
summary(model3_glm_binomial)
#using glm to train the full model with a logit setting
model3_glm_logit <- glm(Attrition ~.,family=binomial(link='logit'),data=train_m3)
#running the test data through the model
predicted_glm_logit <- predict(model3_glm_logit, test_m3, type='response')
predicted_glm_logit <- ifelse(predicted_glm_logit > 0.5,1,0)
summary(model3_glm_logit)
table(test_m3$Attrition, predicted_glm_binomial)
#Checking the prediction accuracy for m3 (glm_binomial)
print((246+17)/300)
table(test_m3$Attrition, predicted_glm_logit)
#Checking the prediction accuracy for m3 (glm_logit)
print((246+17)/300)

#Model4
training_attrition_model4 = training_attrition[,c(2,3,6,9,14,15,16,20,23,25,27,30,31)]  #numcowork is #19
validation_attrition_model4 = validation_attrition[,c(2,3,6,9,14,15,16,20,23,25,27,30,31)]

str(training_attrition_model4)

#simplifining the names
train_m4 <- training_attrition_model4
test_m4 <- validation_attrition_model4
#using glm to train the full model with a binomial setting
model4_glm_binomial <- glm(Attrition ~ ., data = train_m4, family='binomial')
#running the test data through the model
predicted_glm_binomial <- predict(model4_glm_binomial, test_m4, type='response')
predicted_glm_binomial <- ifelse(predicted_glm_binomial > 0.5,1,0)
summary(model4_glm_binomial)
#using glm to train the full model with a logit setting
model4_glm_logit <- glm(Attrition ~.,family=binomial(link='logit'),data=train_m4)
#running the test data through the model
predicted_glm_logit <- predict(model4_glm_logit, test_m4, type='response')
predicted_glm_logit <- ifelse(predicted_glm_logit > 0.5,1,0)
summary(model4_glm_logit)
table(test_m4$Attrition, predicted_glm_binomial)
#Checking the prediction accuracy for m4 (glm_binomial)
print((247+13)/300)
table(test_m4$Attrition, predicted_glm_logit)
#Checking the prediction accuracy for m4 (glm_logit)
print((247+13)/300)




### we can leave this last part for last

#adding the predictions back into the data
validation_attrition$PredictedAttrition_binomial <- predicted_glm_binomial
validation_attrition$PredictedAttrition_logit <- predicted_glm_logit
#check out the predicted data
validation_attrition_compare <- validation_attrition[,c(2,33:34)]
```
Model1: Removing variables that may not make sense (or were redundant) yielded a prediction accuracy of 86.3%.

Model2: Using statistically significant variables (p-value < 0.001) yielded a prediction accuracy of 84.7%.

Model3: Using statistically significant variable (p-value <0.05) yielded a prediction accuracy of 87.7%.

Model4: Using statistically significant variables (p-value <0.05) and also removing possibly redundant variables yielded a prediction accuracy of: 86.67%.